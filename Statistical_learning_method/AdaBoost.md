# [Adaboost算法的原理与推导](https://blog.csdn.net/v_july_v/article/details/40718799)

- 前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数

- **公式推导以及实例说明**



# [Adaboost算法 by 向阳+](https://blog.csdn.net/m0_37407756/article/details/67637400)

**特点：**

- 每次迭代改变的是样本的分布，而不是重复采样（re weight)
- 样本分布的改变取决于样本是否被正确分类

- 最终的结果是弱分类器的加权组合



**优点：**

　　1)adaboost是一种有很高精度的分类器

　　2)可以使用各种方法构建子分类器，adaboost算法提供的是框架

　　3)当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单

　　4)简单，不用做特征筛选

　　5)不用担心overfitting！



**两个特性**

- 错误率上界随着迭代次数的增加，不断下降
- 不会出现过拟合现象



**多分类问题：**

- adaboost M1方法： adaboost组合的若干个弱分类器本身就是多分类的分类器
- adaboost MH方法： 组合的弱分类器仍然是二分类的分类器，将分类label和分类样例组合，生成N个样本，在这个新的样本空间上训练分类器
- 对多分类输出进行二进制编码： 对N个label进行二进制编码，例如用m位二进制数表示一个label，然后训练m个二分类器，在解码时生成m位的二进制数，从而对应到一个label上



**adaBoost算法不需要预先知道弱学习算法学习正确率的下限即弱分类器的误差**





# [Adaboost算法原理分析和实例 by pan_jinquan](https://blog.csdn.net/guyuealian/article/details/70995333)

















