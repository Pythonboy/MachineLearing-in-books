# 一 统计学习方法概论

## 1. 1 统计学习

**统计学习的方法：**

- 监督学习
- 非监督学习
- 半监督学习
- 强化学习



## 1.2 监督学习

**预测任务：**

- 回归问题 ： 连续变量
- 分类问题 ： 有限个离散变量
- 标注问题 ： 变量序列



## 1.3 统计学习三要素

**方法 = 模型 + 策略 + 算法**

### 模型

**条件概率分布 或 决策函数**

### 策略

**选取最优模型：**

- 损失函数 ：度量模型一次预测的好坏
- 风险函数 ： 度量平均意义下的模型预测的好坏

**损失函数：**

- 0-1 损失函数 ： $L(Y, f(X)) = (1 ,Y not f(X) | 0 , Y is f(X))$ *注： 分类问题*
- 平方损失函数： $L(Y, f(X)) = (Y - f(X) )^2$
- 绝对损失函数： $L(Y, f(X)) = |Y - f(X) |$
- 对数损失函数： $L(Y, P(Y|X)) =  - log P(Y|X) $

*损失函数值越小，模型越好*

对于给定的一个训练数据集，模型f(X)关于其的平均损失称为经验风险：$R_{emp}(f) = \frac{\sum L(y_i , f(x_i))}{N}$



**经验风险最小化与结构风险最小化**

1.  **经验风险最小化** ： $min R_{erm}(f) = min \frac{\sum L(y_i , f(x_i))}{N}$ 

 要求要有足够大的样本容量； 当样品容量很小时，容易产生”过拟合“现象；

2. **结构风险最小化** ： 为了防止过拟合，等价于正则化； 结构风险在经验风险的基础之上加上表示模型复杂度的正则化项或罚项； $R_{srm}(f) = \frac{\sum L(y_i , f(x_i))}{N} + \alpha J(f)$ *模型f越复杂，复杂度J(f)越大*；因此：  $min R_{srm}(f) = min \frac{\sum L(y_i , f(x_i))}{N} + \alpha J(f)$



## 1.4 模型评估与模型选择

### 训练误差与测试误差

**统计学习的目的不仅仅是为了能够使学习到的模型对已知数据，更是为了能够对未知数据都有很好的预测能力**

对于给定的两种学习方法，**测试误差小的**学习方法具有更好的预测能力，是更有效的方法。

### 过拟合与模型选择

**过拟合**是指学习时选择的模型所包含的参数过多，以至于出现这一模型对已知数据预测得很好，但是对于未知数据预测得很差的现象。

模型的最终目的是为了**使测试误差达到最小**



## 1.5正则化与交叉验证

1. **正则化** ： 正则化项一般都是模型复杂度的单调递增函数，模型越复杂，正则化值就越大 ； $$min \frac{\sum L(y_i , f(x_i))}{N} + \alpha J(f)$$

   **example : 回归问题** ： L1正则化 （$\alpha |w|$) & L2正则化（$\frac{\alpha}{2} |w|^2$) 

   Occam's razor : *在所有模型中，能够很好地解释已知数据并且十分简单才是最好的模型*

2. **交叉验证** ： 训练集（training set) & 验证集(validation set) & 测试集（test set）

- 简单交叉验证
- S折交叉验证
- 留一交叉验证（S=N）



## 1.6 泛化能力

#### 泛化误差

**泛化能力** 是指由某一种学习方法得到的模型对未知数据的预测能力。

#### 泛化误差上界

通过比较**泛化误差上界**能够比较两种学习方法的优劣性；

**性质：**

- 它是样本容量的函数，当样品容量增加时，泛化上界趋于0
- 它是假设空间容量的函数，当假设空间容量越大，模型越难学，泛化误差上界也就越大



## 1.7 生成模型与判别模型

1. 生成方法由数据学习联合概率分布 P(X , Y) ,然后求出条件概率分布 P(Y | X) 作为预测的模型 ： $$ P( Y | X) = \frac{P(X,Y)}{P(X)}$$ example : 朴素贝叶斯法 和 隐马尔可夫模型 **对于给定X生成输出Y的生成关系**
2. 判别方法由数据直接学习决策函数 f(X) 或者条件概率分布 P(Y | X) 作为预测的模型； example : K近邻 、 感知机、 决策树、 支持向量机 等 **对于给定的X输出怎样的输出Y**



## 1.8 分类问题

评价分类器性能的指标一般为**分类准确率** *（accuracy）* ： 即为0-1损失函数

#### 精准率（precision) & 召回率（recall）

*注：通常把关注的类为正类，其他类为负类*

- TP ： 将正类预测为正类
- FP ： 将负类预测为正类
- TN ： 将负类预测为负类
- FN： 将正类预测为负类

*（注：第二个字母P/N代表你的预测，第一个字母F/T代表你预测的正确性）*

**精准率**

$$precision = \frac{TP}{TP + FP}$$

**召回率**

$$recall = \frac{TP}{TP + FN}$$

**$F_1$ : precision & recall的调和平均值**

$$F_1 = \frac{2TP}{2TP + FP + FN}$$

$$\frac{2}{F_1} = \frac{1}{Precision} + \frac{1}{recall}$$



## 1.9 标注问题

**标注问题的输入是一个观测序列，输出是一个标记序列或状态序列**

评价标注模型的指标与评价分类模型的指标一样，常用的由**标注准确率、精确率、召回率**

统计学习方法： **隐马尔可夫模型、条件随机场**



## 1.10 回归问题

**回归问题用于预测输入变量和输出变量之间的关系**

*可以认为等价于函数拟合*

*一元回归 & 多元回归 | 线性回归 &非线性回归*

最常用的损失函数： **平方损失函数**

求解方法： **最小二乘法**



# 二 感知机

## 2.1 感知机模型

**定义**

$$ f(x) = sign(wx + b)$$

**感知机是一种线性分类模型，属于判别模型**

**超平面S** ： w x + b = 0 ;



## 2.2 感知机学习策略

**损失函数**的选择是误分点数到超平面S的总距离

定义为：

$$L(w,b) = - \sum y_i (w x_i + b)$$



## 2.3 感知机学习算法

最优化方法是**随机梯度下降法**



**公式推导：**

损失函数极小化的解 ： $$min_{w,b} L(w,b) = - \sum y_i(w x_i + b)$$

损失函数的梯度：

$$\nabla _w L(w,b) = - \sum y_ix_i$$

$$\nabla -b L(w,b) = - \sum y_i$$

更新权值和偏置：

$$w = w + \eta y_ix_i$$

$$b = b + \eta y_i$$



#### 感知机学习算法的原始形式

输入： 训练数据集T； 学习率$\eta$（0，1] 

输出： w, b ; 感知机模型 f(x) = sign( w x + b)

1. 选取初值$w_0,b_0$
2. 在训练集中选取数据（$w_i,y_i$)
3. 如果$y_i(w x_i + b) \leq 0$: 更新w ,b
4. 转至（2）， 直至训练集中没有误分类点



#### 算法的收敛性

当训练数据集线性可分时，感知机学习算法是收敛的。感知机算法在训练数据集上的误分类次数k满足不等式：$$k \leq (\frac{R}{\gamma})^2$$

当训练数据集线性可分时，感知机学习算法存在无穷多个解，其解由于不同的初值或不同的迭代顺序而可能有所不同。



#### 感知机学习算法的对偶形式

输入： 线性可分的数据集T ； 学习率 $\eta$ (0,1]

输出：$\alpha$ , b ; 感知机模型 $f(x) = sign( \sum _{j=1}^N \alpha _j y_j x_j x + b)$ ; 其中 $\alpha = (\alpha _1 ,\alpha _2 , ....., \alpha _n)^T$

1.  $\alpha$ = 0 , b = 0
2. 在训练集中选取数据$(x_i,y_i)$
3. 如果$y_i ( \sum _{j=1}^N \alpha _j y_j x_j x + b) \leq 0 $ : $$\alpha _i = \alpha _i + \eta$$ $$b = b + \eta y_i$$
4. 转至（2） 直至没有误分类数据出现





# 三 k 近邻法

k近邻法（KNN）是一种基本的**分类与回归方法**

KNN不具有显式的学习过程；

K值的选择、距离度量和分类决策规则时kNN的三个基本要素

## 3.1 k近邻算法

**算法：**

输入： 训练数据集 

输出： 实例x所属的类y

1. 根据给定的距离度量，在训练集T中找出与x最近邻的k个点，涵盖这k个点的x的邻域记作$N_k(x)$
2. 在$N_k(x)$中根据分类决策规则（如多数表决）决定x的类别y



## 3.2 k近邻模型

模型由3个基本要素——距离度量、k值的选择和分类决策规则决定的

#### 模型

**距离度量、k值的选择和分类决策规则**确定后，对于任何一个新的输入实例，它所属的类唯一地确定

#### 度量距离

$L_p$距离 or Minkowski距离

（p = 1 : 曼哈顿距离 ； p = 2： 欧式距离）

#### k值的选择

if k 较小，估计误差会增大，预测结果对近邻的实例点十分敏感； 即 k值的减小意味着整体模型变得复杂，容易发生过拟合；

if k 较大，学习的近似误差会增大； k值的增大意味着整体的模型会变得简单；

在应用中，k值一般选取一个比较小的数值，通常采用<font color = "red">**交叉验证**</font>的方法来选取最优的k值；

#### 分类决策规则

<font color = "red">**多数表决**</font>

*等价于风险最小化*



# 四 朴素贝叶斯法

## 4.1 朴素贝叶斯法

朴素贝叶斯法对条件概率分布作了条件独立性的假设，即用于分类的特征在类确定的条件下都是条件独立的。

后验概率计算：

$$P(Y =c_k | X = x) = \frac{P(X = x | Y = c_k)P(Y = c_k)}{\sum _k [P(X = x | Y = c_k)P(Y = c+k)]}$$

即：

$$P(Y =c_k | X = x) = \frac{P(Y = c_k) \prod _j P(X^{(j)} = x^{(j)} | Y = c_k)}{\sum _k [P(Y = c+k)  \prod _j P(X^{(j)} = x^{(j)} | Y = c_k)]}$$

则朴素贝叶斯分类器可表示为：

$$ y = f(x) = argmax_{c_k} \frac{P(Y = c_k) \prod _j P(X^{(j)} = x^{(j)} | Y = c_k)}{\sum _k [P(Y = c+k)  \prod _j P(X^{(j)} = x^{(j)} | Y = c_k)]}$$

即：

$$y = argmax_{c_k} P(Y = c_k) \prod _j P(X^{(j)} = x^{(j)} | Y = c_k)$$



## 4.2 朴素贝叶斯法的参数估计

在朴素贝叶斯法中，学习意味着估计$P(Y =c_k)$ 和 $P(X^{(j)} = x^{(j)} | Y = c_k)$

**极大似然估计法**

$$P(Y = c_k) = \frac{\sum_{i=1}^N I(y_i = c_i)}{N} , k = 1,2,3,4.....,N$$



设第j个特征$x^{(j)}$可能的取值集和为${a_{j1},a_{j2}....a_{js}}$， 条件概率：

$$P(X^{(j)} = a_{jl} | Y = c_k) = \frac{\sum _i^N I(x_i^{(j)} = a_{jl} , y_i = c_k)}{\sum _i^N I(y_i = c_k)}$$

*（计算每个特征下的样本的各种可能的取值的条件概率）*

#### 算法：

输入： 训练数据T

输出： 实例X的分类

1. 计算先验概率和条件概率
2. 对于给定的实例计算 $$P(Y = c_k) \prod _j P(X^{(j)} = x^{(j)} | Y = c_k)$$
3. 确定实例的类$$y = argmax_{c_k} P(Y = c_k) \prod _j P(X^{(j)} = x^{(j)} | Y = c_k)$$



## 4.3 贝叶斯估计

用最大似然估计可能会出现所要估计的概率值为0的情况，使分类产生误差，因此可以采用贝叶斯估计：

$P_{\lambda}(X^{(j)} = a_{jl} | Y = c_k) = \frac{\sum _i^N I(x_i^{(j)} = a_{jl} , y_i = c_k) + \lambda}{\sum _i^N I(y_i = c_k) + S_j * \lambda}$

$$P_{\lambda}(Y = c_k) = \frac{\sum_{i=1}^N I(y_i = c_i) + \lambda}{N + K \lambda} , k = 1,2,3,4.....,N$$





# 五 决策树

一种基本的分类与回归方法；

决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程；

优点是 **模型具有可读性，分类速度快**；

决策树的3个步骤： **特征选择，决策树的生成和决策树的修剪**

## 5.1 决策树模型与学习

**决策树定义：**

分类决策树模型是一种描述对实例进行分类的树形结构。决策树是由结点（Node）和有向边（directed edge）组成。结点有两种类型： 内部结点（internal node） 和 叶节点（leaf node）。内部结点表示一个特征或属性，叶节点表示一个类。

决策树的生成只考虑局部最优； 决策树的剪枝则考虑全局最优 

## 5.2 特征选择

特征选择的准则一般是**信息增益或信息增益比**

#### 信息增益

熵（entropy)表示随机变量不确定性的度量，随机变量X的熵定义为:

$$H(p) = - \sum _{i=1}^n p_i log p_i$$

随机变量X给定条件下随机变量Y的条件熵 H（Y|X）：

$$H(Y|X) = \sum _{i=1}^n p_i H(Y | X = x_i)$$

**信息增益**表示得知特征X的信息而使得Y的信息的不确定性减少的程度；

$$g ( D,A) = H(D) - H(D|A)$$

根据信息增益准则的特征选择方法是 ： **对于训练数据集（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征**

#### 信息增益的算法

输入： 训练数据集D和特征A

输出： 特征A对训练数据集D的信息增益g(D,A)

1. 计算数据集D的经验熵 H(D)
2. 计算特征A对数据集D的经验条件熵H(D|A)
3. 计算信息增益

#### 信息增益比

**定义：** 特征A对训练数据集D的信息增益比$g_R(D,A)$定义：$$g_R(D,A) = \frac{g(D,A)}{H_A(D)}$$

其中$$H_A(D) = - \sum _{i=1}^n \frac{|D_i|}{|D|} log_2 \frac{|D_i|}{|D|}$$



## 5.3 决策树的生成

### ID3算法

**算法核心** 是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。

**具体算法：**

从根节点开始，对结点计算所有可能地特征地信息增益，选择信息增益最大地特征作为结点地特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树！

[ID3算法——维基百科](https://zh.wikipedia.org/wiki/ID3%E7%AE%97%E6%B3%95)

### C4.5的生成算法

**C4.5算法对ID3算法进行了改进，在生成的过程中，用信息增益比来选择特征**



## 5.4 决策树的剪枝

通过**剪枝**简化决策树模型，减轻过拟合现象！

通常通过**极小化决策树整体的损失函数或代价函数**来实现

决策树的损失函数：$$C_{\alpha} (T) =  \sum _{t=1}^{|T|} N_t H_t(T) + \alpha |T|$$

$$C_{\alpha} (T) = C (T) + \alpha |T|$$

其中：

$$\alpha \geq 0$$

$C (T) = \sum _{t=1}^{|T|} N_t H_t(T) = - \sum _{t=1}^{|T|} \sum _{k=1}{K} N_{tk} log \frac{N_{tk}}{N_t}$

C(T) 表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型复杂度，参数$\alpha \geq 0$控制两者之间的影响。较大的$\alpha$促使选择**简单的模型**，反之促使选择**复杂的模型**。



## 5.5 CART算法

由**特征选择、树的生成、剪枝**组成

### （一）CART生成

递归地构建二叉决策树，对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择

#### 分类树的生成

基尼指数：

$$ Gini(p) = 1 - \sum _{k=1}{K}P_k^2$$

对于给定的样本集和D，其基尼系数：

$$Gini(D) = 1 - \sum _{k=1}{K} (\frac{|C_k|}{D})^2$$

其中，$C_k$是D中属于第k类的样本子集，K是类的个数

如果样本集和D根据特征A是否取某一个可能值a被分割为$D_1$ & $D_2$，则在特征A的条件下，集和D的基尼系数定义为:    

$$Gini(D,A) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|} Gini(D_2)$$

**基尼指数越大，样本集和的不确定性也就越大**



#### 回归树的生成

- 最小二乘法
- 平方误差最小化准则



### （二）CART剪枝

由两步组成：

1. 首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个子树序列$(T_0,T_1.....T_n)$
2. 然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树



# 六 逻辑回归与最大熵模型

## 6.1 逻辑回归模型

#### 逻辑分布（logistic distribution）

定义： X服从逻辑分布是指X具有下列分布函数和密度函数：

$$F(x) = P(X \leq x) = \frac{1}{1 + e^{-(x - \mu)/\gamma}}$$

$$f(x) = F^'(x) = \frac{e^{-(x - \mu)/\gamma}}{\gamma (1 + e^{-(x - \mu)/\gamma})^2}$$

#### 二项逻辑回归模型

**条件概率分布：**

$$P(Y = 1 | x) = \frac{exp(w x + b)}{1 + exp(w x + b)}$$

$$P(Y = 0 | x) = \frac{1}{1 + exp(w x + b)}$$

对于逻辑回归：

$$\frac{P(Y=1 | x)}{1 - P(Y = 1 | x)} = w x$$

即输出Y= 1 的对数几率是输入x的线性函数

#### 模型参数估计

**对数似然函数：**

$$L(w) = \sum _{i=1}^{N} [y_i (w x_i) - log (1 + exp(w x_i)]$$

对L(w)求最大值，得到w的估计值

#### 多项式逻辑回归模型

$$P(Y = k|x) = \frac{exp(w_k x)}{1 + \sum _{k=1}^{K-1} exp(w_k x)} , k = 1,2,3,......,K-1$$

$$P(Y = K | x) = \frac{1}{1 + \sum _{k=1}^{K-1} exp(w_k x)}$$



## 6.2 最大熵模型

#### 最大熵原理

假设离散变量X的概率分布为P(X)，其熵为：

$$H(P) = - \sum_x P(x) logP(x)$$

在没有更多信息的情况下，那些不确定的部分都是**等可能的**，***最大熵原理通过熵的最大化来表示等可能性***

#### 最大熵模型的定义

联合分布：

$$P'(X = x , Y = y) = \frac{v (X = x , Y = y)}{N}$$

边缘分布:

$$P'(X = x) = \frac{v(X = x)}{N}$$

其中v( )表示输入出现的次数 ， N表示样本容量 ； 

用**特征函数f(x,y)**描述x,y之间的某一个事实，定义为：

$$f(x,y) = 1 ,x & y 满足某一事实 | 0 ，否则$$

**模型学习的约束条件**

$$\sum_{x,y} P'(x)P(y|x)f(x,y) = \sum_{x,y} P'(x,y)f(x,y)$$



**最大熵模型**：

假设满足所有约束条件的模型集合为C，定义在条件概率分布P(Y|X)上的条件熵为：

$$H(P) = - \sum_{x,y} P'(x) P(y|x) log P(y|x)$$

则模型集和C条件熵H(P)最大的模型称为最大熵模型



# 七 支持向量机

SVM 是一种二类分类模型 ；通过某些核，实现非线性分类器

学习策略 : 间隔最大化

## 7.1 线性可分支持向量机与硬间隔最大化

分离超平面对应于方程$\omega x + b = 0$,它由法向量$\omega$和截距b决定； 法向量指向的一侧为正类，另一侧为负类；

#### 线性可分可支持向量机

给定线性可分训练数据集，通过间隔最大化或等价求解相应的凸二次规划问题学习得到的分离超平面为：$$\omega ^* x + b^* = 0$$

以及对应的分类决策函数：

$$f(x) = sign(\omega ^* x + b^*)$$

共称为线性可分支持向量机。

#### 函数间隔和几何间隔

**函数间隔**：

对于给定的训练数据集T以及超平面($\omega$,b)，定义样本点$(x_i,y_i)$关于其的函数间隔为：

$$\gamma ^~ = y_i (\omega x_i + b)$$

定义超平面关于T的函数间隔为其对T中所有样本点中的函数间隔最小值

***函数间隔可以表示分类预测的正确性和确信度***



**几何间隔**

对$\mid\mid\omega\mid\mid = 1$$进行规范化，此时函数间隔就成为几何间隔

$$\gamma _i = y_i (\frac{\omega}{\mid\mid\omega\mid\mid} x_i + \frac{b}{\mid\mid\omega\mid\mid})$$



$$几何间隔 = \frac{函数间隔}{\mid\mid\omega\mid\mid}$$

#### 间隔最大化

**线性可分支持向量机学习算法——最大间隔法**

输入： 线性可分数据集T

输出： 最大间隔分离超平面和分类决策函数

1. 构造并求解约束最优化问题：求得最优解$w^{*},b^{*}$

$$ min_{a,b} = \frac{1}{2}\mid\mid\omega\mid\mid$$

$$s.t.      y_i (\omega x_i + b) - 1 \gep 0, i=1,2,3....N$$

2. 由此得到超平面以及分类决策函数$f(x) = sign(w^{*} x + b^{*})$$

**最大间隔分离超平面的存在唯一性**

**支持向量和间隔边界**

支持向量是使约束条件式等号成立的点： $y_i (\omega x_i + b)  = \pm 1$

设$H_1$是正例点的支持向量，$H_2$是负例点的支持向量，则二者间的距离成为**间隔(margin)**； 间隔依赖于分离超平面的法向量$\omega$,等于$\frac{2}{\mid\mid\omega\mid\mid}$. $H_1$ 、 $H_2$ 称为间隔边界。









